import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from torch.autograd import grad
import os
import time

class AdaptiveTanh(nn.Module):
    """Layer-wise Locally Adaptive Tanh Activation (Jagtap et al. 2020)"""
    def __init__(self, n=10):
        super().__init__()
        self.a = nn.Parameter(torch.ones(1)*n)
        
    def forward(self, x):
        return torch.tanh(self.a * x)

class PINN_LMD(nn.Module):
    """Physics-Informed Neural Network for LMD (Li et al. 2023)"""
    def __init__(self, input_dim=4, hidden_dim=30, output_dim=1):
        super().__init__()
        
        # Register buffers for scaling and normalization
        self.register_buffer('input_mean', torch.zeros(input_dim))
        self.register_buffer('input_std', torch.ones(input_dim))
        self.register_buffer('k_x', torch.ones(1))
        self.register_buffer('k_t', torch.ones(1))
        self.register_buffer('k_u', torch.ones(1))
        
        # Network architecture (30 neurons, 1 residual block)
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        self.hidden1 = nn.Linear(hidden_dim, hidden_dim)
        self.res_block = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            AdaptiveTanh(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.output_layer = nn.Linear(hidden_dim, output_dim)
        
        # Initialize weights (Xavier/Glorot)
        nn.init.xavier_normal_(self.input_layer.weight)
        nn.init.zeros_(self.input_layer.bias)
        nn.init.xavier_normal_(self.hidden1.weight)
        nn.init.zeros_(self.hidden1.bias)
        nn.init.xavier_normal_(self.res_block[0].weight)
        nn.init.zeros_(self.res_block[0].bias)
        nn.init.xavier_normal_(self.res_block[2].weight)
        nn.init.zeros_(self.res_block[2].bias)
        nn.init.xavier_normal_(self.output_layer.weight, gain=0.1)
        nn.init.zeros_(self.output_layer.bias)
        
    def set_scaling_factors(self, Lx, Ly, Lz, t1, t2, T_max):
        """Set scaling factors as per Eq. (12) in paper"""
        self.k_x.data = torch.tensor(1.0/max(Lx, Ly, Lz), dtype=torch.float32)
        self.k_t.data = torch.tensor(1.0/(t1 + t2), dtype=torch.float32)
        self.k_u.data = torch.tensor(1.0/T_max, dtype=torch.float32)
        
    def set_input_stats(self, mean, std):
        """Set input normalization statistics"""
        self.input_mean.data.copy_(torch.as_tensor(mean, dtype=torch.float32))
        self.input_std.data.copy_(torch.as_tensor(std, dtype=torch.float32))
        
    def forward(self, x):
        # Normalize and scale inputs
        x_norm = (x - self.input_mean) / self.input_std
        x_scaled = torch.cat([
            x_norm[:,:3] * self.k_x,  # x,y,z scaling
            x_norm[:,3:4] * self.k_t   # t scaling
        ], dim=1)
        
        # Forward pass
        h = torch.tanh(self.input_layer(x_scaled))
        h = torch.tanh(self.hidden1(h))
        
        # Residual block
        residual = h
        h = self.res_block(h) + residual
        h = torch.tanh(h)
        
        # Output with sigmoid (0-1 range)
        u_scaled = torch.sigmoid(self.output_layer(h))
        
        # Unscale temperature output
        return u_scaled / self.k_u

class LMDPhysics:
    """Physics module implementing LMD-specific equations"""
    def __init__(self, params):
        self.params = params
        self.sigma_sb = 5.67e-8  # Stefan-Boltzmann constant
        self.Lx, self.Ly, self.Lz = params['Lx'], params['Ly'], params['Lz']
        self.t1, self.t2 = params['t1'], params['t2']
        self.Q_max = 5.0e11  # Max laser power density for normalization
        self.T_ref = 3000.0   # Reference temperature for scaling
        
    def thermal_conductivity(self, T):
        """Temperature-dependent k(T) from Table 2"""
        k_low = 2.0e-5*T**2 - 0.0441*T + 49.94  # T < 1773.15K
        k_high = torch.full_like(T, 31.9)        # T >= 1773.15K
        return torch.where(T < 1773.15, k_low, k_high)
        
    def specific_heat(self, T):
        """Temperature-dependent Cp(T) from Table 2"""
        cp_low = 1.04e-4*T**2 - 0.3426*T + 314.2  # T < 1773.15K
        cp_high = torch.full_like(T, 700.0)       # T >= 1773.15K
        return torch.where(T < 1773.15, cp_low, cp_high)
        
    def laser_heat_source(self, x, y, z, t):
        """Gaussian laser heat source from Eq. (4)"""
        v, P = self.params['v'], self.params['P']
        eta, Ra, Rb, Rc = self.params['eta'], self.params['Ra'], self.params['Rb'], self.params['Rc']
        x0, y0, z0 = 0.0, self.Ly/2, 0.0  # Laser path center
        
        # Active only during deposition
        active = (t <= self.t1).float()
        
        # Ellipsoidal Gaussian (Eq. 4)
        r_sq = ((x - (v*t + x0))/Ra)**2 + ((y - y0)/Rb)**2 + ((z - z0)/Rc)**2
        Q = (6*np.sqrt(3)*eta*P/(np.pi*np.sqrt(np.pi)*Ra*Rb*Rc)) * torch.exp(-3*r_sq) * active
        
        return Q / self.Q_max  # Normalized
    
    def compute_derivatives(self, model, x, y, z, t):
        """Compute derivatives via automatic differentiation"""
        x_t = x.clone().requires_grad_(True)
        y_t = y.clone().requires_grad_(True)
        z_t = z.clone().requires_grad_(True)
        t_t = t.clone().requires_grad_(True)
        
        inputs = torch.stack([x_t, y_t, z_t, t_t], dim=1)
        T = model(inputs)
        
        # First derivatives
        dT_dx = grad(T, x_t, grad_outputs=torch.ones_like(T), create_graph=True)[0]
        dT_dy = grad(T, y_t, grad_outputs=torch.ones_like(T), create_graph=True)[0]
        dT_dz = grad(T, z_t, grad_outputs=torch.ones_like(T), create_graph=True)[0]
        dT_dt = grad(T, t_t, grad_outputs=torch.ones_like(T), create_graph=True)[0]
        
        # Second derivatives
        d2T_dx2 = grad(dT_dx, x_t, grad_outputs=torch.ones_like(dT_dx), create_graph=True)[0]
        d2T_dy2 = grad(dT_dy, y_t, grad_outputs=torch.ones_like(dT_dy), create_graph=True)[0]
        d2T_dz2 = grad(dT_dz, z_t, grad_outputs=torch.ones_like(dT_dz), create_graph=True)[0]
        
        return T, dT_dt, dT_dx, dT_dy, dT_dz, d2T_dx2, d2T_dy2, d2T_dz2
    
    def pde_residual(self, model, x, y, z, t):
        """PDE residual from Eq. (1) and (5)"""
        T, dT_dt, dT_dx, dT_dy, dT_dz, d2T_dx2, d2T_dy2, d2T_dz2 = \
            self.compute_derivatives(model, x, y, z, t)
        
        # Material properties
        rho = self.params['rho']
        Cp = self.specific_heat(T)
        k = self.thermal_conductivity(T)
        
        # Heat source term
        Q = self.laser_heat_source(x, y, z, t)
        
        # Heat equation residual (normalized)
        residual = (rho * Cp * dT_dt - k * (d2T_dx2 + d2T_dy2 + d2T_dz2) - Q * self.Q_max)
        return residual / self.Q_max
    
    def ic_residual(self, model, x, y, z, t):
        """Initial condition residual from Eq. (2) and (6)"""
        inputs = torch.stack([x, y, z, t], dim=1)
        T = model(inputs)
        return (T - self.params['T0']) / self.T_ref
    
    def bc_residual(self, model, x, y, z, t):
        """Boundary condition residual from Eq. (3) and (7)"""
        T, _, dT_dx, dT_dy, dT_dz, _, _, _ = self.compute_derivatives(model, x, y, z, t)
        
        # Boundary detection
        tol = 1e-4
        is_x_min = torch.abs(x) < tol
        is_x_max = torch.abs(x - self.Lx) < tol
        is_y_min = torch.abs(y) < tol
        is_y_max = torch.abs(y - self.Ly) < tol
        is_z_min = torch.abs(z) < tol
        is_z_max = torch.abs(z - self.Lz) < tol
        
        # Initialize residual
        bc_residual = torch.zeros_like(T)
        
        # Material properties
        k = self.thermal_conductivity(T)
        h = self.params['h']
        epsilon = self.params['epsilon']
        T0 = self.params['T0']
        
        # Boundary conditions
        q_conv = h * (T - T0)
        q_rad = epsilon * self.sigma_sb * (T**4 - T0**4)
        
        # Apply BCs
        bc_residual = torch.where(is_x_min, dT_dx, bc_residual)  # x=0: dT/dx=0
        bc_residual = torch.where(is_x_max, k*dT_dx + q_conv + q_rad, bc_residual)  # x=Lx
        bc_residual = torch.where(is_y_min | is_y_max, dT_dy, bc_residual)  # y=0, y=Ly
        bc_residual = torch.where(is_z_min, dT_dz, bc_residual)  # z=0
        bc_residual = torch.where(is_z_max, k*dT_dz + q_conv + q_rad, bc_residual)  # z=Lz
        
        return bc_residual / self.Q_max

def generate_training_points(params, n_pde=20000, n_ic=30000, n_bc=10000, device='cpu'):
    """Generate training points with focused sampling (Sec. 4)"""
    Lx, Ly, Lz = params['Lx'], params['Ly'], params['Lz']
    t1, t2 = params['t1'], params['t2']
    t_max = t1 + t2
    v = params['v']
    
    # PDE points (70% laser region, 20% surface, 10% uniform)
    n_laser = int(0.7 * n_pde)
    t_laser = torch.rand(n_laser, device=device).pow(2) * t1  # More early samples
    x_laser = v * t_laser + 0.0005 * torch.randn(n_laser, device=device)
    y_laser = Ly/2 + 0.001 * torch.randn(n_laser, device=device)
    z_laser = 0.0002 * torch.rand(n_laser, device=device).pow(3)
    
    n_surface = int(0.2 * n_pde)
    x_surface = torch.rand(n_surface, device=device) * Lx
    y_surface = torch.rand(n_surface, device=device) * Ly
    z_surface = 0.0001 * torch.rand(n_surface, device=device).pow(4)
    t_surface = torch.rand(n_surface, device=device) * t_max
    
    n_uniform = n_pde - n_laser - n_surface
    x_uniform = torch.rand(n_uniform, device=device) * Lx
    y_uniform = torch.rand(n_uniform, device=device) * Ly
    z_uniform = torch.rand(n_uniform, device=device) * Lz
    t_uniform = torch.rand(n_uniform, device=device) * t_max
    
    # Combine PDE points
    X_pde = torch.stack([
        torch.cat([x_laser, x_surface, x_uniform]),
        torch.cat([y_laser, y_surface, y_uniform]),
        torch.cat([z_laser, z_surface, z_uniform]),
        torch.cat([t_laser, t_surface, t_uniform])
    ], dim=1)
    
    # Initial condition points (t=0)
    X_ic = torch.stack([
        torch.rand(n_ic, device=device) * Lx,
        torch.rand(n_ic, device=device) * Ly,
        torch.rand(n_ic, device=device) * Lz,
        torch.zeros(n_ic, device=device)
    ], dim=1)
    
    # Boundary condition points (6 faces)
    n_per_face = n_bc // 6
    t_bc = torch.rand(n_per_face, device=device) * t_max
    
    # Generate points for each face
    faces = []
    for i in range(6):
        if i == 0:  # x=0
            x = torch.zeros(n_per_face, device=device)
            y = torch.rand(n_per_face, device=device) * Ly
            z = torch.rand(n_per_face, device=device) * Lz
        elif i == 1:  # x=Lx
            x = torch.ones(n_per_face, device=device) * Lx
            y = torch.rand(n_per_face, device=device) * Ly
            z = torch.rand(n_per_face, device=device) * Lz
        elif i == 2:  # y=0
            x = torch.rand(n_per_face, device=device) * Lx
            y = torch.zeros(n_per_face, device=device)
            z = torch.rand(n_per_face, device=device) * Lz
        elif i == 3:  # y=Ly
            x = torch.rand(n_per_face, device=device) * Lx
            y = torch.ones(n_per_face, device=device) * Ly
            z = torch.rand(n_per_face, device=device) * Lz
        elif i == 4:  # z=0
            x = torch.rand(n_per_face, device=device) * Lx
            y = torch.rand(n_per_face, device=device) * Ly
            z = torch.zeros(n_per_face, device=device)
        else:  # z=Lz
            x = torch.rand(n_per_face, device=device) * Lx
            y = torch.rand(n_per_face, device=device) * Ly
            z = torch.ones(n_per_face, device=device) * Lz
        
        faces.append(torch.stack([x, y, z, t_bc], dim=1))
    
    X_bc = torch.cat(faces)
    
    return X_pde, X_ic, X_bc

def train_pinn(model, physics, X_pde, X_ic, X_bc, device, 
              epochs=10000, lbfgs_epochs=16000, print_freq=100):
    """Two-phase training as per paper (Sec. 4)"""
    # Unpack coordinates
    x_pde, y_pde, z_pde, t_pde = X_pde[:,0], X_pde[:,1], X_pde[:,2], X_pde[:,3]
    x_ic, y_ic, z_ic, t_ic = X_ic[:,0], X_ic[:,1], X_ic[:,2], X_ic[:,3]
    x_bc, y_bc, z_bc, t_bc = X_bc[:,0], X_bc[:,1], X_bc[:,2], X_bc[:,3]
    
    # Loss weights (paper values)
    loss_weights = {'pde': 5.0, 'ic': 1.0, 'bc': 10.0}
    
    # Adam optimizer first
    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=500)
    
    # Loss history
    history = {'total': [], 'pde': [], 'ic': [], 'bc': []}
    
    print("Starting Adam optimization...")
    for epoch in range(epochs):
        optimizer.zero_grad()
        
        # Compute residuals
        loss_pde = torch.mean(physics.pde_residual(model, x_pde, y_pde, z_pde, t_pde)**2)
        loss_ic = torch.mean(physics.ic_residual(model, x_ic, y_ic, z_ic, t_ic)**2)
        loss_bc = torch.mean(physics.bc_residual(model, x_bc, y_bc, z_bc, t_bc)**2)
        
        # Weighted total loss
        total_loss = (loss_weights['pde'] * loss_pde + 
                     loss_weights['ic'] * loss_ic + 
                     loss_weights['bc'] * loss_bc)
        
        # Backpropagation
        total_loss.backward()
        optimizer.step()
        scheduler.step(total_loss)
        
        # Record history
        history['total'].append(total_loss.item())
        history['pde'].append(loss_pde.item())
        history['ic'].append(loss_ic.item())
        history['bc'].append(loss_bc.item())
        
        # Print progress
        if (epoch+1) % print_freq == 0:
            print(f"Epoch {epoch+1}/{epochs}: Loss={total_loss.item():.3e} "
                 f"(PDE={loss_pde.item():.3e}, IC={loss_ic.item():.3e}, BC={loss_bc.item():.3e})")
    
    # L-BFGS optimization
    if lbfgs_epochs > 0:
        print("\nStarting L-BFGS optimization...")
        lbfgs_optimizer = optim.LBFGS(model.parameters(), 
                                    max_iter=lbfgs_epochs,
                                    max_eval=lbfgs_epochs*2,  # Allow more evaluations
                                    tolerance_grad=1e-11,      # Tighter tolerance
                                    tolerance_change=1e-14,    # Tighter tolerance
                                    history_size=50,
                                    line_search_fn='strong_wolfe')
        
        # Progress tracking for L-BFGS
        lbfgs_iter = [0]
        
        def closure():
            lbfgs_optimizer.zero_grad()
            loss_pde = torch.mean(physics.pde_residual(model, x_pde, y_pde, z_pde, t_pde)**2)
            loss_ic = torch.mean(physics.ic_residual(model, x_ic, y_ic, z_ic, t_ic)**2)
            loss_bc = torch.mean(physics.bc_residual(model, x_bc, y_bc, z_bc, t_bc)**2)
            total_loss = (loss_weights['pde'] * loss_pde + 
                         loss_weights['ic'] * loss_ic + 
                         loss_weights['bc'] * loss_bc)
            total_loss.backward()
            
            # Record history (only on certain iterations to avoid slowdown)
            if lbfgs_iter[0] % 10 == 0:
                history['total'].append(total_loss.item())
                history['pde'].append(loss_pde.item())
                history['ic'].append(loss_ic.item())
                history['bc'].append(loss_bc.item())
                
                if lbfgs_iter[0] % print_freq == 0:
                    print(f"L-BFGS Iter {lbfgs_iter[0]}: Loss={total_loss.item():.3e} "
                         f"(PDE={loss_pde.item():.3e}, IC={loss_ic.item():.3e}, BC={loss_bc.item():.3e})")
            
            lbfgs_iter[0] += 1
            return total_loss
        
        # Run L-BFGS optimization with manual step to ensure it executes
        try:
            # This explicit step() call will force L-BFGS to run
            lbfgs_optimizer.step(closure)
            print(f"L-BFGS completed with {lbfgs_iter[0]} iterations")
        except Exception as e:
            print(f"L-BFGS optimization failed: {e}")
            print("Check for numerical instabilities or try reducing lbfgs_epochs")
    
    return history

def plot_temperature_field(model, physics, time, plane='xy', save_path=None):
    """Visualize temperature field as in paper figures"""
    model.eval()
    device = next(model.parameters()).device
    
    # Create grid
    n_points = 200
    if plane == 'xy':
        # Top surface (z=0)
        x = torch.linspace(0, physics.Lx, n_points, device=device)
        y = torch.linspace(0, physics.Ly, n_points, device=device)
        X, Y = torch.meshgrid(x, y, indexing='ij')
        Z = torch.zeros_like(X)
        xlabel, ylabel = 'X (m)', 'Y (m)'
    elif plane == 'xz':
        # Vertical slice (y=Ly/2)
        x = torch.linspace(0, physics.Lx, n_points, device=device)
        z = torch.linspace(0, physics.Lz, n_points, device=device)
        X, Z = torch.meshgrid(x, z, indexing='ij')
        Y = torch.ones_like(X) * physics.Ly / 2
        xlabel, ylabel = 'X (m)', 'Z (m)'
    else:
        raise ValueError("plane must be 'xy' or 'xz'")
    
    # Predict temperatures
    grid_points = torch.stack([X.flatten(), Y.flatten(), Z.flatten(), 
                             torch.ones_like(X.flatten()) * time], dim=1)
    with torch.no_grad():
        T = model(grid_points).cpu().numpy().reshape(X.shape)
    
    # Create plot
    plt.figure(figsize=(12, 8))
    
    # Temperature contour
    levels = np.linspace(physics.params['T0'], 3000, 50)
    contour = plt.contourf(X.cpu().numpy(), Y.cpu().numpy(), T, 
                          levels=levels, cmap='inferno')
    
    # Melt pool contour (Tm=1730K)
    if plane == 'xy':
        plt.contour(X.cpu().numpy(), Y.cpu().numpy(), T, 
                   levels=[physics.params['Tm']], colors='cyan', linewidths=2)
        
        # Laser position
        if time <= physics.t1:
            laser_x = physics.params['v'] * time
            laser_y = physics.Ly / 2
            plt.plot(laser_x, laser_y, 'wo', markersize=8, label='Laser position')
            plt.legend()
    
    # Formatting
    plt.colorbar(contour, label='Temperature (K)')
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(f'Temperature Field at t={time:.2f}s (plane={plane})')
    plt.gca().set_aspect('equal')
    plt.grid(True, alpha=0.3)
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

def plot_loss_history(history, save_path=None):
    """
    Plot loss curves similar to Fig. 4 in the paper
    """
    plt.figure(figsize=(10, 6))
    
    # Log-scale for y-axis to match the paper
    epochs = range(len(history['total']))
    
    # Plot all loss components
    plt.semilogy(epochs, history['pde'], label='PDE loss', color='green', linewidth=1.5)
    plt.semilogy(epochs, history['ic'], label='IC loss', color='cyan', linewidth=1.5)
    plt.semilogy(epochs, history['bc'], label='BC loss', color='red', linewidth=1.5)
    plt.semilogy(epochs, history['total'], label='Total loss', color='blue', linewidth=1.5)
    
    # Add Adam/L-BFGS transition marker
    if len(epochs) > 4000:  # Assuming 4000 Adam epochs as in your code
        plt.axvline(x=4000, color='black', linestyle='--', alpha=0.5)
        plt.text(4000+100, max(history['total']), 'Adam â†’ L-BFGS', 
                fontsize=10, rotation=0, verticalalignment='top')
    
    # Formatting to match the paper style
    plt.xlabel('Epochs', fontsize=12)
    plt.ylabel('Training loss (log)', fontsize=12)
    plt.grid(True, which="both", ls="--", alpha=0.3)
    plt.legend(fontsize=10)
    plt.title('Loss curve of the PINN model in the deposition stage', fontsize=14)
    
    # Set y-axis limits similar to the paper
    plt.ylim(min(min(history['bc'])*0.5, 1e-10), max(history['total'][:10])*2)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()

def main():
    # Set device with proper CUDA error handling
    try:
        if torch.cuda.is_available():
            # Test CUDA capabilities
            test_tensor = torch.zeros(1, device='cuda')
            device = torch.device('cuda')
            cuda_name = torch.cuda.get_device_name(0)
            print(f"CUDA is available. Using GPU: {cuda_name}")
        else:
            device = torch.device('cpu')
            print("CUDA is not available. Using CPU.")
    except Exception as e:
        device = torch.device('cpu')
        print(f"Error initializing CUDA: {e}")
        print("Falling back to CPU.")
    
    # Parameters from Table 2 in paper
    params = {
        'rho': 7780.0, 'eta': 0.75, 'P': 2000.0, 'v': 8.0e-3,
        'Ra': 3.0e-3, 'Rb': 3.0e-3, 'Rc': 1.0e-3,
        'h': 20.0, 'epsilon': 0.85, 'T0': 293.15,
        'Tm': 1730.0, 'Ts': 1690.0,
        'Lx': 0.04, 'Ly': 0.02, 'Lz': 0.005,
        't1': 2.0, 't2': 10.0
    }
    
    # Initialize model with device awareness
    try:
        model = PINN_LMD().to(device)
        print(f"Model initialized on: {next(model.parameters()).device}")
        
        # Set scaling factors (Eq. 12)
        model.set_scaling_factors(params['Lx'], params['Ly'], params['Lz'],
                                params['t1'], params['t2'], 3000.0)
        
        # Set input normalization
        input_mean = [params['Lx']/2, params['Ly']/2, params['Lz']/2, params['t1']/2]
        input_std = [params['Lx'], params['Ly'], params['Lz'], params['t1'] + params['t2']]
        model.set_input_stats(input_mean, input_std)
        
        # Initialize physics
        physics = LMDPhysics(params)
        
        # Generate training data with proper device handling
        print("Generating training data...")
        X_pde, X_ic, X_bc = generate_training_points(
            params, n_pde=5000, n_ic=7000, n_bc=3000, device=device)
        
        # Train the model (paper epochs)
        print("Starting training...")
        history = train_pinn(model, physics, X_pde, X_ic, X_bc,
                            device, epochs=5000, lbfgs_epochs=1000)
        
        # Save the trained model
        checkpoint = {
            'model_state_dict': model.state_dict(),
            'params': params,
            'history': history
        }
        torch.save(checkpoint, 'pinn_lmd_model.pt')
        print("Model saved to pinn_lmd_model.pt")
        
        # Plot results
        plot_loss_history(history, save_path='loss_history.png')
        
        # Visualize at key time points
        for t in [0.5, 1.0, 2.0]:
            plot_temperature_field(model, physics, t, plane='xy', 
                                  save_path=f'temp_xy_t{t:.1f}.png')
            plot_temperature_field(model, physics, t, plane='xz',
                                  save_path=f'temp_xz_t{t:.1f}.png')
        
    except Exception as e:
        print(f"Error during model training: {e}")
        import traceback
        traceback.print_exc()
        
        # Try to save any progress if model exists
        try:
            if 'model' in locals() and 'history' in locals():
                checkpoint = {
                    'model_state_dict': model.state_dict(),
                    'params': params,
                    'history': history,
                    'error': str(e)
                }
                torch.save(checkpoint, 'pinn_lmd_model_checkpoint.pt')
                print("Progress saved to pinn_lmd_model_checkpoint.pt")
        except:
            print("Could not save checkpoint.")

if __name__ == "__main__":
    main()
